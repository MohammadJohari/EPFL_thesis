\chapter{Conclusion and Future Work} \label{sec:chapter5}

\section{Conclusion}

In this thesis, we have made significant contributions to the fields of 3D computer vision and scene understanding through the introduction of three novel approaches. Firstly, we propose a novel approach to improve the accuracy of monocular depth estimation from depth sensor raw data. Subsequently, we introduce a new generalizable method that enhances the quality of synthesized images from novel camera poses. Finally, we introduce an efficient dense visual SLAM system that outperforms state-of-the-art methods in both accuracy and efficiency.

 We have presented DepthInSpace in \hyperref[sec:chapter2]{Chapter~\ref{sec:chapter2}} which represents a significant leap forward in the accurate estimation of depth from structured-light sensor data. By integrating optical flow and harnessing information from multiple video frames within a self-supervised framework, DepthInSpace enhances depth estimation accuracy and outperforms existing methods, as evidenced by qualitative and quantitative evaluations across diverse datasets, including synthetic and real-world scenes.

Moving on to our second contribution in \hyperref[sec:chapter3]{Chapter~\ref{sec:chapter3}}, GeoNeRF stands out as a pioneering generalizable method for novel view synthesis. This approach not only achieves state-of-the-art image quality for complex scenes but also eliminates the need for per-scene optimization. Leveraging recent advancements in multi-view architectures and radiance fields, GeoNeRF constructs cascaded cost volumes for source views, which are then aggregated through an attention-based network for synthesizing images from novel poses. Furthermore, we propose promising avenues for future research, suggesting that the incorporation of advanced algorithms to dynamically select nearby views or an adaptive approximation of the optimal number of required cost volumes could further enhance the versatility and efficiency of GeoNeRF.

Lastly, our third contribution in \hyperref[sec:chapter4]{Chapter~\ref{sec:chapter4}}, ESLAM, introduces a top-tier dense visual SLAM by leveraging Neural Radiance Fields to improve both speed and accuracy. Through the innovative replacement of voxel grid representation with axis-aligned feature planes and the adoption of a Truncated Signed Distance Field for scene geometry modeling, ESLAM showcases remarkable advancements in reconstruction and localization accuracy. Notably, our experiments validate that ESLAM improves existing methods' accuracy significantly while running up to one order of magnitude faster.

 Collectively, these three contributions not only advance the state-of-the-art in their respective domains but also set the stage for further research and applications, demonstrating the transformative potential of our innovative approaches in the broader landscape of 3D computer vision research.

\section{Future Work and Recent Advances in Successor Works}

Considering the advancements highlighted in this thesis, numerous exciting opportunities for future research emerge within the domains of depth estimation, novel view synthesis, and dense visual SLAM. It is worth noting that due to the rapid expansion of research in these areas, there are already several concurrent or inspired works documented in the literature as of the time of this writing. This section offers a concise overview of potential extensions, both existing and prospective.


To enhance \textbf{DepthInSpace}, there is potential for additional investigation into expanding the self-supervised framework to include active stereo. This involves utilizing a stereo camera to capture the illuminated pattern, which contributes supplementary information for the algorithm to reason about geometry and occlusion. Notably, this approach could allow for the adoption of a sparser projection pattern, offering advantages in terms of power consumption and efficiency.

An additional avenue of research involves addressing the constraints imposed by the current dataset. Despite our exploration of various synthetic datasets and the evaluation of our method on real-world scenes, the absence of an extensive real dataset remains a limiting factor for the effectiveness of our proposed approach. A potential strategy to alleviate this limitation is to adapt the model to leverage recent advancements in conventional monocular depth estimation. Utilizing large datasets, such as~\cite{ranftl2020towards}, in this domain could provide robust prior information for the network, helping to mitigate the aforementioned issue to some extent.

An alternative enhancement to boost the generalizability of DepthInSpace to out-of-distribution depths involves substituting the single-frame depth regressor network with a cost-volume-based depth estimator network, such as MVSNet~\citep{yao2018mvsnet}. This modification makes the network indifferent to the absolute values of depths or disparities. Consequently, the network becomes capable of functioning on novel datasets where depth values exhibit a significant shift compared to those in the training sets. Such architecture is employed in the concurrent work by~\cite{li2023self}, and a similar idea is explored in another coexisting work, GigaDepth~\citep{schreiberhuber2022gigadepth}. GigaDepth employs a hierarchy of adaptive MLPs to robustly predict depth instead of opting for direct regression through a CNN.

Lastly, exploring the application of structured-light for depth estimation in dynamic scenes represents a natural extension for future research. This introduces novel challenges in comprehending and modeling temporal changes within the scene. Recent concurrent studies conducted by~\cite{qiao2022tide} and~\cite{qiao2023online} have specifically attempted to tackle these challenges.

In our \textbf{GeoNeRF} contribution, a promising avenue for future research involves enhancing efficiency and minimizing computational resource requirements. Despite achieving exceptional quality in the rendered output, GeoNeRF does not rank among the most efficient rendering algorithms. Notably, the attention blocks in the renderer section of GeoNeRF stand out as the most computationally intensive aspect of the algorithm.

To alleviate the computational burden in these blocks, one potential modification involves adaptively filtering out less pertinent input source views. This strategy aims to decrease the number of tokens processed per pixel. Additionally, implementing more efficient transformer attention architectures, such as Perceiver IO~\citep{jaegle2021perceiver} or SegFormer~\citep{xie2021segformer}, could further diminish the computation costs associated with the renderer.

Another research direction to speed up GeoNeRF could be reducing the number of cast rays. More specifically, rendering a low-resolution image using anti-aliasing radiance field approaches~\citep{Barron_2021_ICCV, hu2023tri}, and scaling up the image using a lightweight super-resolution network can significantly reduce the computation costs. Exploiting such a technique along with feature rendering instead of RGB rendering has shown promising progress in the NeRF literature~\citep{huang2023refsr,wang20224k,han2024volume}.

Concurrent or after our research in GeoNeRF, the following studies also explored the field of generalizable 3D understanding. WaveNeRF~\cite{xu2023wavenerf} integrates wavelet frequency decomposition into MVS and NeRF to achieve generalizable yet high-quality synthesis without any per-scene optimization. NeuRay~\citep{liu2022neural} exploits MVS to detect occluded source views and proposes an implicit occlusion-aware feature aggregation. \cite{du2023learning} and \cite{suhail2022generalizable} eliminate alpha compositing in ray casting by using an attention-based aggregation of the image-based features and directly regressing the output color to save computation. LIRF~\citep{huang2023local} supports generalizable NeRF rendering at arbitrary scales by proposing an anti-aliasing architecture. \cite{liu2024one} and \citep{liu2023onetwo} introduce a method that generates a 3D textured mesh from a single image exploiting diffusion models. And ContraNeRF~\citep{yang2023contranerf} explores synthetic-to-real generalization of radiance fields.

Lastly, while our \textbf{ESLAM} approach has made significant strides in enhancing the accuracy and efficiency of implicit visual SLAM, it still falls short of the real-time performance and precise localization capabilities of traditional SLAM methods. Specifically, techniques like loop closure detection, global bundle adjustment, and second-order optimization of camera poses, which have proven their effectiveness in traditional SLAM, need to be incorporated and tailored to the unique framework of implicit neural-based visual SLAM systems like ours. In successor literature, MIPS-Fusion~\citep{tang2023mips} and NGEL-SLAM~\citep{mao2023ngel} propose to learn implicit sub-maps for the environment and introduce submap-level loop closure detection and correction. \cite{haghighi2023neural} completely decouple tracking and mapping representation and use the traditional ORB-SLAM~3~\citep{campos2021orb} as a tracking backbone.

Another compelling extension to our research in ESLAM would be to investigate techniques for reducing the computational burden associated with addressing the forgetting problem. Our current representation, heavily reliant on feature planes, is particularly susceptible to this challenge. A promising strategy to tackle this issue lies in leveraging recent advancements in the field of continual learning. Techniques such as  Elastic Weight Consolidation (EWC)~\citep{kirkpatrick2017overcoming}, which preserve the knowledge of old tasks while learning new ones, could be seamlessly integrated into our ESLAM optimization scheme, effectively alleviating the need to re-sample pixels from previous key-frames.

Concurrent or after our work in ESLAM, the following works also explored the field of implicit representation in visual SLAM. Vox-Fusion~\citep{yang2022vox} and Point-SLAM~\citep{sandstrom2023point} address the memory constraints of previous methods by employing a sparse octree structure or a sparse point cloud, respectively, for the 3D map. DNS SLAM~\citep{li2023dns} and SNI-SLAM~\citep{zhu2023sni} incorporate semantic information in the SLAM pipeline. NID-SLAM~\cite{xu2024nid} and DDN-SLAM~\citep{li2024ddn} extend the visual SLAM to dynamic scenes. \cite{liu2023efficient} introduces map fusion of multiple SLAM agents. NICER-SLAM~\citep{zhu2023nicer} and HI-SLAM~\citep{zhang2023hi} investigate dense mapping in a monocular RGB SLAM setting while EN-SLAM~\citep{qu2023implicit} integrates event sensor data with the RGBD SLAM system. More recently, with the emergence of Gaussian Splatting~\citep{kerbl20233d}, there has been a growing interest in integrating this novel efficient 3D representation into the dense visual SLAM pipeline~\citep{matsuki2023gaussian,yan2023gs,yugay2023gaussian,keetha2023splatam,huang2023photo}.

Overall, our contributions have opened several new avenues for research in 3D computer vision. By leveraging our novel techniques and insights, researchers can develop more efficient and performant algorithms for tasks such as depth estimation, novel view synthesis, 3D reconstruction, scene understanding, and SLAM. These advancements have the potential to broaden the reach and impact of 3D computer vision across various domains, from robotics and augmented reality to autonomous driving and medical imaging.