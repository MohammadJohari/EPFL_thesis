\chapter{Introduction}

In recent times, the emergence of deep learning has brought about a significant transformation in the field of computer vision, marking a shift in how machines interpret visual information. The fusion of artificial intelligence and neural network structures, particularly the multi-layered configurations of deep neural networks, has fundamentally changed the landscape of visual processing. At the core of deep learning is its ability to independently learn hierarchical representations from raw data, making it a powerful tool for addressing intricate visual tasks that were previously considered insurmountable. The impact of this approach extends beyond mere image recognition, influencing various sectors such as robotics, healthcare, security, and entertainment.

In the context of deep learning for computer vision, a pivotal moment occurred in 2012 when \cite{krizhevsky2012imagenet} introduced their work, now known as AlexNet, a groundbreaking convolutional neural network (CNN) architecture. This innovative architecture marked a new era by significantly surpassing traditional methods in the ImageNet Large Scale Visual Recognition Challenge~\citep{ILSVRC15}. This accomplishment not only showcased the effectiveness of deep learning in image classification but also ignited a substantial increase in research and development within the field. Successive architectures, including VGGNet~\citep{Simonyan15}, GoogLeNet~\citep{szegedy2015going}, ResNet~\citep{he2016deep}, and ViT~\citep{kolesnikov2021an}, played a crucial role in advancing accuracy and scalability, solidifying deep learning as the cornerstone of modern computer vision.

As we explore the vast realm of deep learning, it is crucial to acknowledge its impact on diverse applications where the ability to interpret visual information has become essential. Autonomous systems, driven by deep learning algorithms, now navigate intricate environments with unprecedented precision and adaptability~\citep{chib2023recent}. In medical imaging, deep learning aids in identifying anomalies, improving diagnostic accuracy, and potentially revolutionizing patient care~\citep{suganyadevi2022review}. Biometric systems, supported by advanced neural networks, redefine security protocols~\citep{jadhav2022review}, while augmented reality experiences benefit from the seamless integration of deep learning for object recognition and scene understanding~\citep{ghasemi2022deep}.

The interaction between deep learning and computer vision goes beyond task optimization; instead, it signifies a symbiotic relationship that continually evolves to address emerging challenges.

Deep learning has significantly impacted not only traditional computer vision tasks but has also brought about transformative changes in the field of 3D computer vision and scene understanding. The incorporation of deep learning methods into the three-dimensional domain signifies a logical progression in artificial intelligence evolution and introduces numerous possibilities to enhance machine perceptual capabilities.

Moving into the third dimension introduces a new layer of complexity, extending beyond the traditional boundaries of two-dimensional image processing. The unique nature of three-dimensional data requires advanced techniques capable of decoding spatial relationships and reconstructing scenes with a nuanced understanding of the complexities inherent in a multidimensional world. This shift necessitates models to address challenges related to depth perception, volumetric understanding, and the intricate interplay of objects within a spatial context. Deep learning, known for its capacity to autonomously learn hierarchical representations from raw data, plays a crucial role in addressing these challenges. It provides a robust framework for machines to develop a nuanced understanding of spatial structures and intricate relationships within the volumetric domain.

In the realm of 3D computer vision, challenges go beyond conventional image recognition as algorithms must now navigate and interpret the intricacies of spatial dimensions. The emphasis shifts from merely identifying present objects to determining their precise locations in three-dimensional space. This shift requires departing from traditional methodologies and encourages the development of novel approaches that leverage the power of deep neural networks to extract insights from volumetric data and multiple perspectives~\citep{maturana2015voxnet, qi2017pointnet, li2018pointcnn}.

In recent years, 3D computer vision has emerged as a dynamic and rapidly evolving field, revolutionizing the way machines perceive and interact with the three-dimensional world. This interdisciplinary domain intersects computer science, mathematics, and optics, aiming to replicate human-like depth perception in machines. From autonomous navigation to augmented reality applications, 3D computer vision tasks have found diverse applications across various industries. In the following, a glimpse into some examples of the prominent 3D computer vision tasks are provided.

\vspace{1ex}
\noindent\textbf{Object Recognition and Detection.} Object Recognition and Detection is a pivotal area in computer vision, which involves the identification and localization of objects within a three-dimensional space. This field has witnessed significant advancements, with methods leveraging point cloud data, depth information, and sophisticated algorithms. PointNet~\citep{qi2017pointnet} marked a milestone by directly processing point clouds for recognition tasks. Building on this, PointNet++~\citep{qi2017pointnet++} and Frustum PointNets~\citep{qi2018frustum} proposed incremental improvements. Recent advances in 3D object recognition and detection have been marked by the integration of deep learning and large-scale datasets. Notable contributions include Diffusion-SS3D~\citep{ho2023diffusion}, which addresses the limitation of the availability of large-scale 3D annotations by exploiting pseudo-labels, and MonoNeRD~\citep{xu2023mononerd}, introducing a Neural Radiance Field-based object detection pipeline. These works highlight the ongoing progress in leveraging neural networks to enhance accuracy and efficiency in 3D object analysis.

\vspace{1ex}
\noindent\textbf{Depth Estimation.} Depth estimation, predicting the distance of each pixel from the camera, is crucial for scene understanding or reconstruction. Monodepth2~\citep{godard2019digging} is one of the pioneer work in the domain which proposes a robust self-supervised approach for depth estimation from monocular images. Numerous researchers have been following this fundamental line of research in 3D domain recently, including but not limited to~\cite{zhao2023gasmono, shao2023nddepth, yang2023gedepth, zhou2023two, piccinelli2023idisc}.

\vspace{1ex}
\noindent\textbf{Object or Scene Reconstruction.} Creating 3D models from 2D images or RGB-D images is the aim of reconstruction approaches. Pixel2Mesh~\citep{wang2018pixel2mesh} and Pix2Vox~\citep{xie2019pix2vox} introduce methods leveraging neural networks for generating 3D mesh models from a single image. More recent work like the ones by \cite{yang2023single} and ~\cite{zhang2022monocular} attempt to improve the generalizability of the reconstruction to unseen object categories. 3D reconstruction is not limited to objects in the literature. Methods like neuralRecon~\citep{sun2021neuralrecon}, TransformerFusion~\citep{bozic2021transformerfusion}, Manhattan-SDF~\citep{guo2022neural}, SceneRF~\citep{cao2023scenerf}, and Uni-3D~\citep{zhang2023uni} show promising result in large-scale scene reconstruction from multi-view inputs.

\vspace{1ex}
\noindent\textbf{3D Pose Estimation.} Determining the spatial configuration of objects or humans in a scene is the task of 3D pose estimation. DeepPose~\citep{toshev2014deeppose} pioneers a deep learning approach for estimating human pose in 3D from 2D images. More recent advanced methods for pose estimation can be found in the works by \cite{zhang20233d} and \cite{zhou2023deep} which are robust to occlusions thanks to 3D understanding of the context.

\vspace{1ex}
\noindent\textbf{Novel View Synthesis.}
Novel view synthesis is crucial for generating new perspectives of a scene and has attracted an unprecedented attention over the past few years. NeRF~\citep{mildenhall2020nerf} is a groundbreaking paper introducing Neural Radiance Fields, a method for synthesizing novel views with impressive realism. Shortly after NeRF, numerous follow-up papers improved its quality~\citep{Barron_2021_ICCV, hu2023tri}, inference speed~\citep{fridovich2022plenoxels, garbin2021fastnerf}, training efficiency~\citep{sun2022direct, muller2022instant}, and applicability~\citep{meshry2019neural, park2021nerfies, chan2022efficient}. The primary drawback of NeRF lies in its inefficiency. To address this limitation, a novel 3D representation called Gaussian Splatting~\citep{kerbl20233d} has emerged. Gaussian Splatting serves as a bridge between NeRF's high-quality view-dependent rendering and the hardware-friendly, efficient classical rasterization approach. Soon after its introduction, it quickly evolved and found uses in various improvements and applications, like dynamic scenes~\citep{luiten2023dynamic,yang2023deformable}, generative models~\citep{chen2023text,tang2023dreamgaussian}, anti-aliasing~\citep{yu2023mip}, and relighting~\citep{gao2023relightable}.

\vspace{1ex}
\noindent\textbf{Simultaneous Localization and Mapping (SLAM).}:  is a fundamental technology in robotics and computer vision, playing a crucial role in enabling machines to understand and navigate their surroundings. It involves the simultaneous process of creating a map of an unknown environment while determining the precise location of the observer within that environment. SLAM has applications ranging from autonomous vehicles and drones to augmented reality, contributing significantly to the development of intelligent systems capable of robustly interacting with and adapting to the world around them. ORB-SLAM2~\citep{mur2017orb}, an open source visual SLAM based on traditional computer vision techniques, is still a leading approach for localization accuracy. Over the past years, deep learning has contributed to many parts of the SLAM algorithms~\citep{mokssit2023deep}. With advent of NeRF, the idea of exploiting implicit representation for SLAM was explored in iMAP~\citep{sucar2021imap} and NICE-SLAM~\citep{zhu2022nice}. Such implicit representation can lead to high-quality 3D reconstruction of the environment. Although these approaches are significantly slower than the traditional methods, they demonstrate promising quality in 3D reconstruction and opened a new line of research in the SLAM area.

This thesis encompasses contributions in the fields of depth estimation, novel view synthesis, and visual SLAM. Section~\ref{sec:c1_outline} outlines our contributions in these domains in detail.

\section{Thesis Outline and Contributions} \label{sec:c1_outline}

The following three chapters in this dissertation are based on three conference proceedings articles~\citep{johari2021depthinspace,johari2022geonerf, johari2023eslam}. In each chapter, we investigate a 3D computer vision task independently, and the chapters can be read in any order. However, the flow of the study unfolds as outlined below.