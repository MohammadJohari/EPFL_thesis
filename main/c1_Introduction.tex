\chapter{Introduction and Thesis Overview}

\section{Introduction}

In recent times, the emergence of deep learning has brought about a significant transformation in the field of computer vision, marking a shift in how machines interpret visual information. The fusion of artificial intelligence and neural network structures, particularly the multi-layered configurations of deep neural networks, has fundamentally changed the landscape of visual processing. At the core of deep learning is its ability to independently learn hierarchical representations from raw data, making it a powerful tool for addressing intricate visual tasks that were previously considered insurmountable. The impact of this approach extends beyond mere image recognition, influencing various sectors such as robotics, healthcare, security, and entertainment.

In the context of deep learning for computer vision, a pivotal moment occurred in 2012 when \cite{krizhevsky2012imagenet} introduced their work, now known as AlexNet, a groundbreaking convolutional neural network (CNN) architecture. This innovative architecture marked a new era by significantly surpassing traditional methods in the ImageNet Large Scale Visual Recognition Challenge~\citep{ILSVRC15}. This accomplishment not only showcased the effectiveness of deep learning in image classification but also ignited a substantial increase in research and development within the field. Successive architectures, including VGGNet~\citep{Simonyan15}, GoogLeNet~\citep{szegedy2015going}, ResNet~\citep{he2016deep}, and ViT~\citep{dosovitskiy2021an}, played a crucial role in advancing accuracy and scalability, solidifying deep learning as the cornerstone of modern computer vision.

As we explore the vast realm of deep learning, it is crucial to acknowledge its impact on diverse applications where the ability to interpret visual information has become essential. Autonomous systems, driven by deep learning algorithms, now navigate intricate environments with unprecedented precision and adaptability~\citep{chib2023recent}. In medical imaging, deep learning aids in identifying anomalies, improving diagnostic accuracy, and potentially revolutionizing patient care~\citep{suganyadevi2022review}. Biometric systems, supported by advanced neural networks, redefine security protocols~\citep{jadhav2022review}, while augmented reality experiences benefit from the seamless integration of deep learning for object recognition and scene understanding~\citep{ghasemi2022deep}.

The interaction between deep learning and computer vision goes beyond task optimization; instead, it signifies a symbiotic relationship that continually evolves to address emerging challenges.

Deep learning has significantly impacted not only traditional computer vision tasks but has also brought about transformative changes in the field of 3D computer vision and scene understanding. The incorporation of deep learning methods into the three-dimensional domain signifies a logical progression in artificial intelligence evolution and introduces numerous possibilities to enhance machine perceptual capabilities.

Moving into the third dimension introduces a new layer of complexity, extending beyond the traditional boundaries of two-dimensional image processing. The unique nature of three-dimensional data requires advanced techniques capable of decoding spatial relationships and reconstructing scenes with a nuanced understanding of the complexities inherent in a multidimensional world. This shift necessitates models to address challenges related to depth perception, volumetric understanding, and the intricate interplay of objects within a spatial context. Deep learning, known for its capacity to autonomously learn hierarchical representations from raw data, plays a crucial role in addressing these challenges. It provides a robust framework for machines to develop a nuanced understanding of spatial structures and intricate relationships within the volumetric domain.

In the realm of 3D computer vision, challenges go beyond conventional image recognition as algorithms must now navigate and interpret the intricacies of spatial dimensions. The emphasis shifts from merely identifying present objects to determining their precise locations in three-dimensional space. This shift requires departing from traditional methodologies and encourages the development of novel approaches that leverage the power of deep neural networks to extract insights from volumetric data and multiple perspectives~\citep{maturana2015voxnet, qi2017pointnet, li2018pointcnn}.

In recent years, 3D computer vision has emerged as a dynamic and rapidly evolving field, revolutionizing the way machines perceive and interact with the three-dimensional world. This interdisciplinary domain intersects computer science, mathematics, and optics, aiming to replicate human-like depth perception in machines. From autonomous navigation to augmented reality applications, 3D computer vision tasks have found diverse applications across various industries. In the following, a glimpse into some examples of the prominent 3D computer vision tasks is provided.

\vspace{1ex}
\noindent\textbf{Object Recognition and Detection.} Object Recognition and Detection is a pivotal area in computer vision, which involves the identification and localization of objects within a three-dimensional space. This field has witnessed significant advancements, with methods leveraging point cloud data, depth information, and sophisticated algorithms. PointNet~\citep{qi2017pointnet} marked a milestone by directly processing point clouds for recognition tasks. Building on this, PointNet++~\citep{qi2017pointnet++} and Frustum PointNets~\citep{qi2018frustum} proposed incremental improvements. Recent advances in 3D object recognition and detection have been marked by the integration of deep learning and large-scale datasets. Notable contributions include Diffusion-SS3D~\citep{ho2023diffusion}, which addresses the limitation of the availability of large-scale 3D annotations by exploiting pseudo-labels, and MonoNeRD~\citep{xu2023mononerd}, introducing a Neural Radiance Field-based object detection pipeline. These works highlight the ongoing progress in leveraging neural networks to enhance accuracy and efficiency in 3D object analysis.

\vspace{1ex}
\noindent\textbf{Depth Estimation.} Depth estimation, predicting the distance of each pixel from the camera, is crucial for scene understanding or reconstruction. Monodepth2~\citep{godard2019digging} is one of the pioneering works in the domain that proposes a robust self-supervised approach for depth estimation from a mixture of monocular and stereo images. Numerous researchers have been following this fundamental line of research in the 3D domain recently, including but not limited to the studies in~\cite{zhao2023gasmono}, \cite{shao2023nddepth}, \cite{yang2023gedepth}, \cite{zhou2023two}, and \cite{piccinelli2023idisc}. In addition to depth sensing from RGB images, another intriguing line of research involves depth estimation from the raw data of active sensors, such as Time-of-Flight~\citep{li2022deltar}, LiDAR~\citep{bartoccioni2023lidartouch}, and structured-light~\citep{riegler2019connecting}.

\vspace{1ex}
\noindent\textbf{Object or Scene Reconstruction.} Creating 3D models from 2D images or RGB-D images is the aim of reconstruction approaches. Pixel2Mesh~\citep{wang2018pixel2mesh} and Pix2Vox~\citep{xie2019pix2vox} introduce methods leveraging neural networks for generating 3D mesh models from a single image. More recent work like the ones by \cite{yang2023single} and ~\cite{zhang2022monocular} attempt to improve the generalizability of the reconstruction to unseen object categories. 3D reconstruction is not limited to objects in the literature. Methods like neuralRecon~\citep{sun2021neuralrecon}, TransformerFusion~\citep{bozic2021transformerfusion}, Manhattan-SDF~\citep{guo2022neural}, SceneRF~\citep{cao2023scenerf}, and Uni-3D~\citep{zhang2023uni} show promising result in large-scale scene reconstruction from multi-view inputs.

\vspace{1ex}
\noindent\textbf{3D Pose Estimation.} Determining the spatial configuration of objects or humans in a scene is the task of 3D pose estimation. DeepPose~\citep{toshev2014deeppose} pioneers a deep learning approach for estimating human pose in 3D from 2D images. More recent advanced methods for pose estimation can be found in the works by \cite{zhang20233d} and \cite{zhou2023deep} which are robust to occlusions thanks to 3D understanding of the context.

\vspace{1ex}
\noindent\textbf{Novel View Synthesis.}
Novel view synthesis is crucial for generating new perspectives of a scene and has attracted unprecedented attention over the past few years. NeRF~\citep{mildenhall2020nerf} is a groundbreaking paper introducing Neural Radiance Fields, a method for synthesizing novel views with impressive realism. Shortly after NeRF, numerous follow-up papers improved its quality~\citep{Barron_2021_ICCV, hu2023tri}, inference speed~\citep{fridovich2022plenoxels, garbin2021fastnerf}, training efficiency~\citep{sun2022direct, muller2022instant}, and applicability~\citep{meshry2019neural, park2021nerfies, chan2022efficient}. The primary drawback of NeRF lies in its inefficiency. To address this limitation, a novel 3D representation called Gaussian Splatting~\citep{kerbl20233d} has emerged. Gaussian Splatting serves as a bridge between NeRF's high-quality view-dependent rendering and the hardware-friendly, efficient classical rasterization approach. Soon after its introduction, it quickly evolved and found uses in various improvements and applications, like dynamic scenes~\citep{luiten2023dynamic,yang2023deformable}, generative models~\citep{chen2023text,tang2023dreamgaussian}, anti-aliasing~\citep{yu2023mip}, and relighting~\citep{gao2023relightable}.

\vspace{1ex}
\noindent\textbf{Simultaneous Localization and Mapping (SLAM).} SLAM is a fundamental technology in robotics and computer vision, playing a crucial role in enabling machines to understand and navigate their surroundings. It involves the simultaneous process of creating a map of an unknown environment while determining the precise location of the observer within that environment. SLAM has applications ranging from autonomous vehicles and drones to augmented reality, contributing significantly to the development of intelligent systems capable of robustly interacting with and adapting to the world around them. ORB-SLAM2~\citep{mur2017orb}, an open-source visual SLAM based on traditional computer vision techniques, is still a leading approach for localization accuracy. Over the past years, deep learning has contributed to many parts of the SLAM algorithms~\citep{mokssit2023deep}. With the advent of NeRF, the idea of exploiting implicit representation for SLAM was explored in iMAP~\citep{sucar2021imap} and NICE-SLAM~\citep{zhu2022nice}. Such implicit representation can lead to the high-quality 3D reconstruction of the environment. Although these approaches are significantly slower than the traditional methods, they demonstrate promising quality in 3D reconstruction and opened a new line of research in the SLAM area.

This thesis encompasses contributions in the fields of depth estimation, novel view synthesis, and visual SLAM. Section~\ref{sec:c1_outline} outlines our contributions in these domains in detail.

\section{Thesis Outline and Contributions} \label{sec:c1_outline}

The following three chapters in this thesis are based on three conference proceedings articles~\citep{johari2021depthinspace,johari2022geonerf, johari2023eslam}. In each chapter, we investigate a 3D computer vision task independently, and the chapters can be read in any order. However, the flow of the study unfolds as outlined below.

In our initial investigation, we focused on a specific challenge related to 3D understanding exclusively within the camera frustum. This undertaking aligns with the established domain of monocular depth estimation within the research framework of 3D computer vision. In \textbf{\hyperref[sec:chapter2]{Chapter~\ref{sec:chapter2}}}, which is based on the work by~\cite{johari2021depthinspace}, we introduce \textbf{DepthInSpace}, a self-supervised deep-learning approach designed for depth estimation through structured-light cameras, addressing the growing demand for embedded depth sensors in contemporary smartphones. With the advent of structured-light cameras, depth sensing has become feasible with basic algorithms implementable on devices with computational constraints in real time. While traditional methods like block matching and semi-global matching have been employed by devices like Kinect V1~\citep{martinez2013kinect} and Intel RealSense~\citep{keselman2017intel}, learning-based approaches in this domain are limited.

The proposed DepthInSpace method leverages optical flow estimates derived from ambient information across multiple video frames to guide the training of a single-frame depth estimation network. This innovative approach aids in preserving edges and mitigating over-smoothing challenges, making depth estimation more effective. Additionally, we propose a technique to fuse data from multiple video frames, enhancing the accuracy of depth maps and minimizing artifacts, particularly in occluded areas. In another study, we demonstrate the efficacy of using the estimated fused depth maps as a self-supervision signal to refine a single-frame depth estimation network, resulting in an overall improvement in performance.

The models are thoroughly evaluated and compared with state-of-the-art counterparts using synthetic and newly introduced real datasets. With the lack of large-scale, precise ground-truth data, our end-to-end training of a deep neural network in a self-supervised manner becomes crucial. The implementation code, training procedure, and datasets are publicly available at \href{https://www.idiap.ch/paper/depthinspace}{https://www.idiap.ch/paper/depthinspace}, facilitating further exploration and development in the field of self-supervised depth estimation.

In our subsequent investigation, we extended our focus beyond the camera frustum, delving into the realm of 3D sensing using multi-view data obtained either from a single object or densely sampled views from a limited camera trajectory of a real-world scene. This research aligns with the well-recognized area of novel view synthesis. \textbf{\hyperref[sec:chapter3]{Chapter~\ref{sec:chapter3}}}, which is based on the work by~\cite{johari2022geonerf}, introduces \textbf{GeoNeRF}, a novel and generalizable approach to photorealistic novel view synthesis leveraging neural radiance fields. Comprising two main stages, our method incorporates a geometry reasoner and a renderer. The geometry reasoner initiates the process by constructing cascaded cost volumes for nearby source views, facilitating sophisticated occlusion reasoning through a Transformer-based attention mechanism. Utilizing these cost volumes, the renderer employs classical volume rendering techniques to infer geometry and appearance, producing detailed images. Notably, this architecture excels in gathering information from consistent source views.

Our approach addresses a critical limitation of Neural Radiance Fields (NeRF)~\citep{mildenhall2020nerf}, which requires scene-specific training, leading to time-consuming per-scene optimization with densely captured images. Recent approaches aiming to generalize NeRF to new scenes often fall short of understanding scene geometry and occlusions, resulting in undesirable artifacts. Building upon the foundation of MVSNeRF~\citep{chen2021mvsnerf}, we introduce key enhancements. The geometry reasoner utilizes cascaded cost volumes trained in a semi-supervised manner to obtain fine and high-resolution priors for conditioning the renderer. The renderer combines an attention-based model, handling information from diverse source views, with an auto-encoder network that aggregates information along a ray. Additionally, our method efficiently detects and excludes occluded views for each point in space, further enhancing rendering quality.

Moreover, GeoNeRF allows straightforward fine-tuning for a single scene, yielding competitive results compared to per-scene optimized neural rendering methods with significantly reduced computational costs. Experimental evaluations demonstrate GeoNeRF's superior performance over state-of-the-art generalizable neural rendering models across synthetic and real datasets. Lastly, as an extension, we propose an alternate model adaptable to RGBD images, leveraging depth information commonly available from depth sensors.

In summary, GeoNeRF advances the field of novel view synthesis by addressing the limitations of existing methods, offering a robust and efficient solution for generalizable neural rendering. The implementation source code and visualization videos are accessible at \href{https://www.idiap.ch/paper/geonerf}{https://www.idiap.ch/paper/geonerf}.

In our last contribution, we investigated a challenge involving the unrestricted movement of a camera within a real-world setting. This investigation is situated within the established and conventional domain of Simultaneous Localization and Mapping (SLAM) within the field of 3D computer vision research. \textbf{\hyperref[sec:chapter4]{Chapter~\ref{sec:chapter4}}}, which is based on the work by~\cite{johari2023eslam}, introduces \textbf{ESLAM}, a novel and efficient method that adapts implicit neural representation to SLAM systems. ESLAM processes RGB-D frames sequentially, gradually reconstructing the scene while simultaneously estimating the current camera position. Traditional SLAM systems primarily focus on localization accuracy, while recent learning-based dense visual SLAM methods provide global 3D maps with reasonable but limited reconstruction accuracy. Inspired by NeRF's capacity to reason about large-scale scene geometry, ESLAM builds upon NeRF-based dense SLAM methods like iMAP~\citep{sucar2021imap} and NICE-SLAM~\citep{zhu2022nice}. Unlike iMAP's single MLP for geometry representation or NICE-SLAM's voxel grid storage for occupancies, ESLAM employs multi-scale axis-aligned feature planes to learn implicit Truncated Signed Distance Field (TSDF). This choice of 3D representation in ESLAM converges faster, delivers higher-quality reconstruction, and reduces the memory footprint growth rate.

Benchmarking on three challenging datasets validates ESLAM's superior performance, showcasing over 50\% improvements in 3D reconstruction and camera localization accuracy compared to state-of-the-art methods. ESLAM achieves this while running up to $\times$10 faster and without the need for pre-training. The method's inherent smoothness, attributed to the representation with feature planes, produces high-quality smooth surfaces without explicit smoothness loss functions. The findings position ESLAM as an efficient and accurate dense visual SLAM method with broad applicability. The implementation source code and visualization videos are accessible at: \href{https://www.idiap.ch/paper/eslam}{https://www.idiap.ch/paper/eslam}

Finally, in \textbf{\hyperref[sec:chapter5]{Chapter~\ref{sec:chapter5}}}, we summarize our findings and propose new directions for future research.